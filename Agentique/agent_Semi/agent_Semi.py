#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# pylint: disable=method-hidden
# pylint: disable=no-member
"""
AgentSemi - Orchestrateur Central et Gestionnaire de Flux (Runtime)
Module principal qui initialise le syst√®me, maintient l'√©tat de la session et pilote la boucle de pens√©e.

Architecture :
    - Pattern : Orchestrateur centralis√© (Hub & Spoke).
    - Ex√©cution : Synchrone pour la g√©n√©ration, Asynchrone (Threading) pour la persistance et l'analyse.
    - I/O : Streaming via Generator (yield) pour une latence faible en frontend.

Responsabilit√©s Techniques :
    1. **Initialization** : Injection de d√©pendances et ordre de chargement critique des sous-agents.
    2. **Cognitive Loop** : Pipeline Intention -> RAG -> Prompting -> Inf√©rence -> Outils.
    3. **Tool Routing** : Parsing des sorties JSON du LLM et dispatch vers les fonctions internes.
    4. **Background Tasks** : D√©l√©gation des I/O lourds (sauvegarde, indexation code) √† des threads d√©mons.
"""

import json
import re
import time
import requests
from scipy import stats
import yaml
import uuid
from datetime import datetime
from dataclasses import asdict, is_dataclass
import threading
from typing import List, Dict, Optional, Any, Callable, TYPE_CHECKING
from pathlib import Path

if TYPE_CHECKING:
    from flask_socketio import SocketIO  # Pour que VS Code comprenne le type
from agentique.base.META_agent import AgentBase
from agentique.base.contrats_interface import (
    Action,
    Categorie,
    StatsBase,
    Sujet,
    SearchMode,
    Interaction,
    MetadataFichier,
    MetadataPipeline,
    ResultatIntention,
    ResultatRecherche,
    ResultatContexte,
    ResultatJuge,
    StandardPrompt,
    StandardPromptCode,
    WebSearchPrompt,
    ProtocolePrompt,
    ManualContextCodePrompt,
    MemorySearchPrompt,
    CartographyPrompt,
    FileInspectionPrompt,
    StagingReviewPrompt,
    ModificateursCognitifs,
    Souvenir,
    CodeChunk,
    CustomJSONEncoder,
    PlanExecution,
    MemorySearchFirstPrompt,
)

from agentique.sous_agents_gouvernes.agent_Memoire.agent_Memoire import AgentMemoire
from agentique.sous_agents_gouvernes.agent_Parole.agent_Parole import AgentParole
from agentique.sous_agents_gouvernes.agent_Parole.moteurs.moteur_llm import MoteurLLM
from agentique.sous_agents_gouvernes.agent_Parole.moteurs.moteur_mini_llm import (
    MoteurMiniLLM,
)
from agentique.sous_agents_gouvernes.agent_Juge.agent_Juge import AgentJuge
from agentique.sous_agents_gouvernes.agent_Reflexor.agent_Reflexor import AgentReflexor
from agentique.sous_agents_gouvernes.agent_Recherche.agent_Recherche import (
    AgentRecherche,
)
from agentique.sous_agents_gouvernes.agent_Contexte.agent_Contexte import AgentContexte
from agentique.sous_agents_gouvernes.agent_Code.agent_Code import AgentCode
from agentique.sous_agents_gouvernes.agent_Code.code_extractor_manager import (
    CodeExtractorManager,
)
from agentique.Semi.classes_cognitives import IntentionDetector
from agentique.sous_agents_gouvernes.agent_Memoire.traitement_brute_persistante import (
    ProcesseurBrutePersistante,
)
from agentique.sous_agents_gouvernes.agent_Memoire.moteur_vecteur import MoteurVectoriel


class AgentSemi(AgentBase):
    def __init__(self, get_cache=None, get_lock=None, socketio=None):
        super().__init__(nom_agent="AgentSemi")

        """
        Contr√¥leur principal du runtime SecondMind.

        G√®re le cycle de vie complet d'une requ√™te utilisateur. Maintient les objets
        transverses (Cache, Lock, SocketIO) et assure la coh√©rence des donn√©es entre
        les diff√©rents moteurs (LLM, Vectoriel, Recherche).

        Attributes:
            current_session_id (str): UUID de la session active (pour le suivi conversationnel).
            active_plan (PlanExecution): √âtat courant du plan d'action (pass√© de prompt en prompt).
            fichiers_actifs (Set[str]): Liste des fichiers "√©pingl√©s" dans le contexte courant (Working Set).
        """
        # ------------------------------------------------------
        # D√©pendances (Cache / Monitoring)
        # ------------------------------------------------------
        self.socketio = socketio
        self.get_cache = get_cache
        self.get_lock = get_lock

        # 2. Initialisation des composants (M√©thodes extraites)
        self._initialiser_moteurs()
        self._initialiser_sous_agents()
        self._initialiser_outils_systeme()
        self._initialiser_agent_code()

        # 3. Configuration Callback & √âtat
        self._setup_callbacks_viewer()
        self._initialiser_etat_session()

        # 4. D√©marrage des processus de fond
        self._lancer_processus_demarrage()

        self.logger.info("‚úÖ AgentSemi initialis√© (Refactoris√©).")

    def _initialiser_outils_systeme(self):
        # On instancie le nouveau Manager (Outil stateless)
        self.code_extractor = CodeExtractorManager()

        # ------------------------------------------------------
        # Initialisation des Moteurs
        # ------------------------------------------------------

    def _initialiser_moteurs(self):
        self.moteur_llm = MoteurLLM()
        self.moteur_mini_llm = MoteurMiniLLM()
        self.moteur_vectoriel = MoteurVectoriel()
        self.processeur_batch = ProcesseurBrutePersistante(llm_engine=self.moteur_llm)

        # =====================================================
        # Initialisation des Agents (Ordre Strict)
        # =====================================================

    def _initialiser_sous_agents(self):
        """
        Instancie les agents et injecte les d√©pendances crois√©es.

        ‚ö†Ô∏è ORDRE CRITIQUE :
        1. Recherche (Base I/O)
        2. Memoire (D√©pend de Recherche + Moteurs)
        3. Reflexor/Juge (D√©pendent de Memoire/Recherche)
        4. Parole (D√©pend de tout le monde pour construire le prompt)

        Modifie l'√©tat interne de l'instance (self.agent_*).
        """
        self.agent_recherche = AgentRecherche()
        self.agent_recherche.moteur_vectoriel = self.moteur_vectoriel  # Injection

        self.agent_memoire: AgentMemoire = AgentMemoire(
            agent_recherche=self.agent_recherche, moteur_vectoriel=self.moteur_vectoriel
        )
        self.agent_recherche.agent_memoire = self.agent_memoire

        self.agent_reflexor = AgentReflexor(
            agent_memoire=self.agent_memoire,
            agent_recherche=self.agent_recherche,
            moteur_llm=self.moteur_llm,
            moteur_vectoriel=self.moteur_vectoriel,
        )
        self.agent_juge = AgentJuge(
            agent_recherche=self.agent_recherche, moteur_mini_llm=self.moteur_mini_llm
        )
        self.agent_contexte = AgentContexte(
            agent_recherche=self.agent_recherche, agent_juge=self.agent_juge
        )
        self.agent_parole: AgentParole = AgentParole(
            agent_contexte=self.agent_contexte,
            agent_semi=self,
            get_cache=self.get_cache,
            get_lock=self.get_lock,
        )

        self.intention_detector = IntentionDetector()

        # --- H. INJECTION TARDIVE POUR DEEP RESEARCH ---
        # AgentRecherche a besoin du LLM pour l'outil avanc√©, mais il a √©t√© cr√©√© avant.
        # On initialise l'outil maintenant que le moteur est dispo.
        from agentique.sous_agents_gouvernes.agent_Recherche.recherche_web import (
            RechercheWeb,
        )

        self.agent_recherche.outil_web = RechercheWeb(self.moteur_llm)
        self.logger.info("‚úÖ Outil RechercheWeb inject√©.")

    def _initialiser_agent_code(self):
        """Initialise le cerveau du code."""
        try:
            self.agent_code = AgentCode()  # ‚úÖ Nouvelle classe
            self.logger.info("‚úÖ AgentCode connect√©.")
        except Exception as e:
            self.logger.log_error(f"‚ö†Ô∏è √âchec init AgentCode: {e}")
            self.agent_code = None

        # =================================================================
        # üîß CORRECTIF PROMPT VIEWER : UNIVERSEL & SOCKET.IO
        # =================================================================

    def _setup_callbacks_viewer(self):
        """Configure le callback pour le Prompt Viewer (SocketIO)."""

        def update_viewer_callback(prompt_str):
            full_raw_prompt = (
                prompt_str if isinstance(prompt_str, str) else "Format Invalide"
            )

            cache = self.get_cache()
            lock = self.get_lock()

            with lock:
                cache["raw_prompt"] = full_raw_prompt
                cache["timestamp"] = datetime.now().isoformat()

            if self.socketio:
                try:
                    self.socketio.emit(
                        "refresh_prompt_viewer",
                        {"timestamp": datetime.now().isoformat(), "status": "updated"},
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur √©mission SocketIO: {e}")

        self.agent_parole._prompt_callback = update_viewer_callback

    def _initialiser_etat_session(self):
        """Initialise les variables d'√©tat de session."""
        self.current_session_id = str(uuid.uuid4())
        self.current_message_turn = 0
        self.derniere_classification: Optional[ResultatIntention] = None
        self.derniere_interaction = None
        self.dernier_code_hash = None
        self.system_instructions = self.agent_parole.recuperer_instruction(
            "instructions_systeme"
        )
        self.active_plan = PlanExecution(objectif_global="")  # Utilise la dataclass
        # NOUVEAU : La liste des fichiers "ouverts" dans l'IDE mental de Semi
        self.fichiers_actifs = set()

    def _lancer_processus_demarrage(self):
        """
        Boot Sequence : Proc√©dures de d√©marrage √† froid.

        1. **Continuit√©** : Recharge les 10 derniers messages pour le contexte imm√©diat.
        2. **Maintenance** : V√©rifie si un batch de vectorisation est en retard (ProcesseurBrutePersistante).
        3. **Identit√©** : V√©rifie/G√©n√®re le r√©sum√© syst√®me initial.
        """

        # 1. Continuit√© de Session (Gouvernance)
        try:
            dernier_historique_chat = self.agent_contexte.get_historique_nouveau_chat()
            if dernier_historique_chat:
                self.agent_contexte.historique_conversation = dernier_historique_chat
                self.logger.info(
                    f"‚úÖ Continuit√© √©tablie: {len(dernier_historique_chat) // 2} √©changes charg√©s."
                )
        except Exception as e:
            self.logger.log_error(f"Erreur chargement continuit√©: {e}")

        # 2. V√©rification Batch Vectorisation
        self._verifier_batch_au_demarrage()

        # 3. Proprioception (R√©sum√© Syst√®me)
        try:
            dossier_semi = Path(self.auditor.get_path("agent_dir"))
            resume_path = dossier_semi / "etat_systeme_resume.md"

            if not resume_path.exists():
                self.logger.info(
                    "üå± Premier lancement : G√©n√©ration de l'identit√© syst√®me..."
                )
                threading.Thread(
                    target=self.actualiser_resume_systeme, daemon=True
                ).start()
        except Exception as e:
            self.logger.log_warning(
                f"Impossible de v√©rifier le r√©sum√© syst√®me au d√©marrage : {e}"
            )

    # =========================================================================
    # üïµÔ∏è‚Äç‚ôÇÔ∏è TRACEUR D'INVESTIGATION (NOUVEAU)
    # =========================================================================
    def _tracer_etape_investigation(
        self, etape: str, prompt_interne: str, reponse_llm: str, outil_resultat: dict
    ):
        """
        Enregistre les √©tapes interm√©diaires de la boucle de recherche.
        Ne pollue pas la m√©moire, c'est du log pur pour le d√©bogage humain.
        """
        # On cr√©e un fichier de log d√©di√© par jour
        date_str = datetime.now().strftime("%Y-%m-%d")
        nom_log = f"trace_investigation_{date_str}.md"

        # On r√©cup√®re le chemin des logs via l'auditor
        dossier_logs = self.auditor.get_path("logs")
        if not dossier_logs:
            return  # Si pas de logs configur√©s, on sort

        chemin_log = Path(dossier_logs) / nom_log
        timestamp = datetime.now().strftime("%H:%M:%S")

        # Extraction du raisonnement (souvent dans le JSON de r√©ponse LLM)
        raisonnement = reponse_llm
        try:
            # Tentative d'extraction propre si c'est du JSON
            if "{" in reponse_llm:
                # On cherche le bloc JSON
                json_part = reponse_llm[
                    reponse_llm.find("{") : reponse_llm.rfind("}") + 1
                ]
                data = json.loads(json_part)
                # On cherche les cl√©s standards de raisonnement
                raisonnement = (
                    data.get("analyse")
                    or data.get("thought")
                    or data.get("reasoning")
                    or reponse_llm
                )
        except:
            pass  # Si √ßa fail, on garde le texte brut

        bloc_log = (
            f"\n## üïµÔ∏è‚Äç‚ôÇÔ∏è √âtape : {etape} ({timestamp})\n"
            f"**üß† Raisonnement :**\n> {str(raisonnement)[:1000]}\n\n"
            f"**üì§ Prompt Interne (Envoy√© au LLM) :**\n```text\n{prompt_interne[:2000]} ... [Tronqu√©]\n```\n\n"
            f"**üì• R√©sultat Outil (Re√ßu) :**\n```json\n{json.dumps(outil_resultat, ensure_ascii=False, indent=2)[:2000]} ... [Tronqu√©]\n```\n"
            f"---\n"
        )

        try:
            with open(chemin_log, "a", encoding="utf-8") as f:
                f.write(bloc_log)
        except Exception as e:
            self.logger.log_warning(f"‚ö†Ô∏è Impossible de tracer l'investigation : {e}")

        # ------------------------------------------------------
        # M√©thode Principale de Pens√©e
        # ------------------------------------------------------

    def penser(
        self,
        prompt: str,
        interaction_id: str = None,  # <-- Optionnel (G√©n√©r√© si None)
        session_id: str = None,  # <-- Optionnel (Prend self.current... si None)
        message_turn: int = None,  # <-- Optionnel
        stream: bool = False,
        search_mode: str = "auto",
        historique_brut: Optional[List[str]] = None,
        enable_thinking: bool = False,
        archive_history: Optional[List[dict]] = None,
    ):
        """
        Boucle principale d'inf√©rence (Main Loop).

        Pipeline d'ex√©cution :
        1. **Pre-Flight** : V√©rification des commandes syst√®me (ex: "+1", "!!!", "nouveau chat").
        2. **Routing** : D√©tection d'intention (Sujet/Action) via Regex ou LLM l√©ger.
        3. **Retrieval** : Collecte du contexte (RAG standard + RAG Code + Recherche Web forc√©e).
        4. **Prompting** : Construction dynamique du prompt via AgentParole.
        5. **Inference** : Streaming de la r√©ponse du LLM.
        6. **Tool Use** : Si du JSON est d√©tect√©, interruption du stream, ex√©cution de l'outil, et r√©cursion.
        7. **Post-Processing** : Lancement du thread asynchrone de sauvegarde.

        Args:
            prompt (str): Input utilisateur brut.
            stream (bool): Si True, yield les tokens en temps r√©el via SocketIO/HTTP.

        Yields:
            str: Tokens de texte ou signaux de contr√¥le.
        """

        # --- ‚è±Ô∏è D√âBUT MOUCHARD ---
        t_start = time.time()
        logs_perf = []

        def tick(label):
            logs_perf.append(f"{label}: {time.time() - t_start:.2f}s")

        # -------------------------

        # --- GESTION DES ID & CONTINUIT√â ---
        if interaction_id is None:
            interaction_id = str(uuid.uuid4())

        # ‚úÖ CORRECTION : Si pas d'ID re√ßu de l'interface, on utilise la session interne de Semi.
        # C'est ce qui assure la continuit√© de la m√©moire vive.
        if session_id is None:
            session_id = self.current_session_id

        # On garde le compteur de tours synchronis√©
        if message_turn is None:
            message_turn = self.current_message_turn

        # (On a SUPPRIM√â le bloc "is_stateless" qui vidait 'historique_brut = []')

        # --- CHARGEMENT HISTORIQUE ARCHIV√â ---
        if archive_history:
            historique_plat = [msg["content"] for msg in archive_history]
            self.agent_contexte.historique_conversation = historique_plat
            self.logger.info(
                f"üìÇ Historique archiv√© charg√©: {len(archive_history) // 2} √©changes"
            )
        # -------------------------------------

        correlation_id = self.logger.set_correlation_id()
        self.logger.info(f"Nouvelle requ√™te [{correlation_id}] : {prompt[:50]}...")

        # M√©triques Log (Volatiles)
        meta_pipeline = MetadataPipeline(interaction_id=str(uuid.uuid4()))

        print("DEBUG: 1. Penser d√©marr√©")
        # ==========================================================
        # 1. Gestion des Commandes Rapides (+1, -1, Piste Rapide)
        # ==========================================================
        resultat_commande = self._gerer_commandes_systeme(prompt, stream)

        print("DEBUG: 2. Commandes pass√©es")
        # ------------------------------------------------------
        # --- NOUVEAU CHAT (premier message) -------------------
        # ------------------------------------------------------
        if resultat_commande == "NOUVEAU_CHAT":
            prompt_texte = self.agent_parole.prompt_premier_chat(prompt)

            # G√©n√©ration directe
            response = ""
            for part in self.moteur_llm.generer_stream(prompt_texte):
                response += part
                if stream:
                    yield part

            self.agent_contexte.mettre_a_jour_historique(prompt, response)
            if not stream:
                yield response
            return
        print("DEBUG: 2.1 Commande nouveau chat pass√©")
        # Autres commandes (Feedback, etc.)
        if resultat_commande:
            if "yield" in resultat_commande:
                yield from resultat_commande["yield"]
                return
            elif "response" in resultat_commande:
                yield resultat_commande["response"]
                return
        print("DEBUG: 2.2 Autres commandes pass√©es")
        # ------------------------------------------------------
        # 2. Gestion des Pistes Forc√©es (Web, M√©moire)
        # ------------------------------------------------------
        # ‚úÖ CORRECTION : On v√©rifie explicitement le mode AVANT d'appeler le g√©n√©rateur
        if search_mode == "web":
            self.logger.info(f"üöÄ Mode Recherche Web Forc√©e activ√©.")
            # On consomme le g√©n√©rateur
            for item in self._handle_forced_search(prompt, search_mode):
                yield item
            # On arr√™te le processus ici car c'est une demande sp√©cifique
            return
        print("DEBUG: 3.0 Handled Forced Search Pass√©")
        # ==========================================================
        # 3. D√âTECTION D'INTENTION (Le Router)
        # ==========================================================
        # On le fait MAINTENANT pour savoir si on a besoin de l'enqu√™teur
        tick("Avant Intention")
        resultat_intention = self.intention_detector.intention_detector(
            prompt, historique_brut=historique_brut
        )
        tick("Apr√®s Intention")

        print(f"DEBUG: 3. Intention d√©tect√©e: {resultat_intention.sujet}")
        # ------------------------------------------------------
        # 4. Pr√©paration du Pipeline Principal (RAG)
        # ------------------------------------------------------
        mode_enum = SearchMode.NONE
        if search_mode == "web":
            mode_enum = SearchMode.WEB
        elif search_mode == "manual_context":
            mode_enum = SearchMode.CONTEXTE_MANUEL

        modificateurs = ModificateursCognitifs(
            activer_cot=False, enable_thinking=enable_thinking, search_mode=mode_enum
        )
        # ------------------------------------------------------
        # 5. RECHERCHE & CONTEXTE (Avec l'intention d√©j√† calcul√©e)
        # ------------------------------------------------------
        resultat_recherche = (
            self.agent_recherche.recherche_contexte_memoire_vectorielle(
                query=prompt, intention=resultat_intention
            )
        )
        tick("Apr√®s Recherche Vectorielle+Boost")

        # NOTE : Si AgentContexte ne g√®re pas les r√®gles, il faudrait les appeler ici.
        # Mais supposons que AgentContexte fait son travail d'agr√©gation.

        resultat_contexte = self.agent_contexte.recuperer_contexte_intelligent(
            resultat_intention=resultat_intention, resultat_recherche=resultat_recherche
        )
        tick("Apr√®s Tri Contexte")

        # üî¥ Injection du PROTOCOLE ALERTE si actif
        if getattr(self, "active_protocol_override", None):
            protocole_souv = Souvenir(
                contenu=self.active_protocol_override,
                titre="PROTOCOLE_ALERTE",
                type="regle",
                score=999.0,
            )
            resultat_contexte.regles_actives.insert(0, protocole_souv)

        print("DEBUG: 5. Recherche finie")
        # ------------------------------------------------------
        # 6. RAG CODE (Canal D√©di√©)
        # ------------------------------------------------------
        liste_code_chunks: List[CodeChunk] = []  # Typage strict

        # Regex simplifi√©e pour d√©clenchement
        import re

        trigger_code = False
        if self.agent_code:
            # On cherche des indices de fichiers ou de structure technique
            if re.search(r"([a-zA-Z0-9_]+)\.(py|md|yaml|json)", prompt) or re.search(
                r"(code|fonction|classe|script|bug|erreur)", prompt, re.IGNORECASE
            ):
                # Appel √† l'AgentCode
                raw_results = self.agent_code.fournir_contexte(prompt)

                # Conversion des r√©sultats bruts en CodeChunk typ√©s
                if raw_results:
                    trigger_code = True
                    for item in raw_results:
                        # 1. Extraction Contenu Robuste
                        contenu = ""
                        if hasattr(item, "contenu"):
                            contenu = item.contenu
                        elif hasattr(item, "code_summary"):
                            contenu = item.code_summary

                        # --- ‚úÖ AJOUT : PASS-THROUGH DES ERREURS ---
                        # Si c'est une erreur technique, on bypass le filtre de longueur
                        is_error = getattr(item, "type", "") == "erreur_technique"

                        # FILTRE : Si le contenu est vide ou < 10 caract√®res (sauf si erreur), on jette
                        if not is_error and (not contenu or len(contenu.strip()) < 10):
                            continue

                        # 2. Extraction Nom (Gestion du Squelette/Souvenir)
                        # Souvenir utilise 'titre', ContexteCode utilise 'name'
                        nom_fichier = "Inconnu"
                        if hasattr(item, "titre"):
                            nom_fichier = item.titre
                        elif hasattr(item, "name"):
                            nom_fichier = item.name
                        elif hasattr(item, "chemin"):
                            nom_fichier = item.chemin

                        liste_code_chunks.append(
                            CodeChunk(
                                contenu=contenu,
                                chemin=nom_fichier,  # Maintenant le nom sera correct (ex: SQUELETTE_DYNAMIQUE)
                                type=getattr(item, "type", "snippet"),
                                langage="python",
                            )
                        )

        # ------------------------------------------------------
        # 6-BIS. INJECTION FICHIERS ACTIFS (Continuit√© Session)
        # ------------------------------------------------------
        # On ajoute les fichiers "√©pingl√©s" par les tours pr√©c√©dents pour √©viter l'amn√©sie
        chunks_actifs = []
        fichiers_a_charger = getattr(self, "fichiers_actifs", [])

        if fichiers_a_charger:
            self.logger.info(f"üìÇ Injection contexte actif : {fichiers_a_charger}")

            # On v√©rifie la pr√©sence de l'outil de lecture
            outil = getattr(self.agent_recherche, "outil_recherche_memoire", None)

            if outil:
                for fichier in fichiers_a_charger:
                    try:
                        # Lecture via la m√©thode unifi√©e (celle utilis√©e par rechercher_memoire)
                        content = outil.lire_fichier_complet(fichier)

                        if content:
                            # Cr√©ation du Chunk avec typage conforme pour AgentParole
                            chunks_actifs.append(
                                CodeChunk(
                                    contenu=content,
                                    chemin=fichier,
                                    type="fichier_actif",  # Permet √† Parole d'appliquer le formatage sp√©cial
                                    langage="python",
                                )
                            )
                    except Exception as e:
                        self.logger.log_warning(
                            f"‚ö†Ô∏è Impossible de relire le fichier actif {fichier}: {e}"
                        )
            else:
                self.logger.log_error(
                    "‚ùå outil_recherche_memoire non disponible pour l'injection active."
                )

        # ------------------------------------------------------
        # ‚úÖ 7. CR√âATION DU PROMPT (MAPPING STRICT)
        # ==========================================================

        prompt_final_obj = None

        # --- A. MODE MANUEL (Priorit√© Absolue) ---
        if modificateurs.search_mode == SearchMode.CONTEXTE_MANUEL:
            self.logger.info("üö® MODE INJECTION CODE MANUEL ACTIV√â.")
            slots_list = (
                historique_brut
                if isinstance(historique_brut, list)
                else [str(historique_brut)]
            )
            code_joint = (
                "\n\n".join(slots_list).strip() if slots_list else "# Aucun code fourni"
            )

            prompt_final_obj = ManualContextCodePrompt(
                prompt_original=prompt,
                instructions_contexte_manuel=self.agent_parole.recuperer_instruction(
                    "instructions_contexte_manuel"
                ),
                contexte_manuel=code_joint,
                intention=resultat_intention,
                historique=resultat_contexte.historique,
                regles=resultat_contexte.regles_actives,
                fichiers_readme=resultat_contexte.fichiers_readme,
                modificateurs=modificateurs,
            )

        # --- B. MODE CARTOGRAPHIE (Nouveau) ---
        elif next(
            (
                s
                for s in resultat_contexte.contexte_memoire
                if s.type == "cartographie_projet"
            ),
            None,
        ):
            self.logger.info("üó∫Ô∏è MODE D√âTECT√â : CARTOGRAPHIE")
            souvenir_map = next(
                (
                    s
                    for s in resultat_contexte.contexte_memoire
                    if s.type == "cartographie_projet"
                ),
                None,
            )
            resume = self.agent_parole._recuperer_resume_systeme()

            prompt_final_obj = CartographyPrompt(
                prompt_original=prompt,
                instructions_cartographie=self.config.get("prompts", {}).get(
                    "instructions_cartographie", ""
                ),
                cartographie_projet=souvenir_map.contenu,
                plan_de_bataille=[resume],
                intention=resultat_intention,
            )

        # --- C. MODE INSPECTION FICHIER (Nouveau) ---
        # Si on a un fichier technique charg√© ET qu'on veut analyser/coder
        elif next(
            (
                s
                for s in resultat_contexte.contexte_memoire
                if s.type in ["fichier_technique", "fichier_brut"]
            ),
            None,
        ) and resultat_intention.categorie in [
            Categorie.ANALYSER,
            Categorie.CODER,
            Categorie.AGENT,
        ]:
            souvenir_fichier = next(
                (
                    s
                    for s in resultat_contexte.contexte_memoire
                    if s.type in ["fichier_technique", "fichier_brut"]
                ),
                None,
            )
            self.logger.info(f"üîß MODE D√âTECT√â : INSPECTION ({souvenir_fichier.titre})")
            resume = self.agent_parole._recuperer_resume_systeme()

            prompt_final_obj = FileInspectionPrompt(
                prompt_original=prompt,
                instructions_inspection=self.config.get("prompts", {}).get(
                    "instructions_inspection", ""
                ),
                fichier_en_cours=souvenir_fichier,
                notes_precedentes=resume,
                intention=resultat_intention,
            )

        # --- D. MODE REVIEW (Nouveau) ---
        elif (
            resultat_intention.categorie == Categorie.PLANIFIER
            and "staging" in prompt.lower()
        ):
            self.logger.info("‚úÖ MODE D√âTECT√â : STAGING REVIEW")
            resume = self.agent_parole._recuperer_resume_systeme()
            prompt_final_obj = StagingReviewPrompt(
                prompt_original=prompt,
                instructions_review=self.config.get("prompts", {}).get(
                    "instructions_review", ""
                ),
                etat_staging_actuel=resume,
                derniere_action="V√©rification demand√©e",
                intention=resultat_intention,
            )

        # --- E. MODE CODE STANDARD ---
        elif (trigger_code and liste_code_chunks) or chunks_actifs:
            self.logger.info(
                f"üíª MODE CODE ACTIV√â : {len(liste_code_chunks)} RAG + {len(chunks_actifs)} Actifs."
            )
            prompt_final_obj = StandardPromptCode(
                prompt_original=prompt,
                instructions_code_prompt=self.agent_parole.recuperer_instruction(
                    "instructions_code_prompt"
                )
                or "Tu es un expert Python.",
                modificateurs=modificateurs,
                intention=resultat_intention,
                historique=resultat_contexte.historique,
                regles=resultat_contexte.regles_actives,
                fichiers_readme=resultat_contexte.fichiers_readme,
                code_chunks=liste_code_chunks + chunks_actifs,
            )

        # --- F. MODE STANDARD (D√©faut) ---
        else:
            prompt_final_obj = StandardPrompt(
                prompt_original=prompt,
                instructions_systeme=self.agent_parole.recuperer_instruction(
                    "instructions_systeme"
                ),
                modificateurs=modificateurs,
                intention=resultat_intention,
                historique=resultat_contexte.historique,
                contexte_memoire=resultat_contexte.contexte_memoire,
                regles=resultat_contexte.regles_actives,
                fichiers_readme=resultat_contexte.fichiers_readme,
            )

        tick("7. Prompt Construit")
        self.derniere_classification = prompt_final_obj.intention
        # ------------------------------------------------------
        # 8. G√©n√©ration (Appel AgentParole -> LLM)
        # ==========================================================
        final_response_text = ""
        llm_success = True
        prompt_texte = self.agent_parole.construire_prompt_llm(prompt_final_obj)
        tick("8. Envoi au Moteur LLM...")

        t_gen_start = time.time()
        first_token_received = False
        buffer_detection = ""
        check_json_done = False
        is_hidden_json_mode = False

        response_generator = self.moteur_llm.generer_stream(prompt_texte)

        try:
            for token in response_generator:
                if not token:
                    continue
                if not first_token_received:
                    ttft = time.time() - t_gen_start
                    tick(f"‚ö° TTFT: {ttft:.2f}s")
                    first_token_received = True

                final_response_text += token

                # BUFFER JSON
                if stream:
                    if not check_json_done:
                        buffer_detection += token
                        if len(buffer_detection) > 50:
                            if re.match(r"^\s*({|```json)", buffer_detection):
                                is_hidden_json_mode = True
                            else:
                                yield buffer_detection
                            check_json_done = True
                    else:
                        if not is_hidden_json_mode:
                            yield token

            if stream and not check_json_done and not is_hidden_json_mode:
                yield buffer_detection

        except Exception as e:
            self.logger.log_error(
                f"[{correlation_id}] Erreur g√©n√©ration LLM: {e}", exc_info=True
            )
            final_response_text = "D√©sol√©, une erreur interne est survenue."
            llm_success = False
            if stream:
                yield final_response_text

        # ==========================================================
        # 9. TRAITEMENT DU JSON (Post-G√©n√©ration) & ROUTAGE OUTILS
        # ==========================================================
        if final_response_text:
            # 1. Nettoyage et Parsing Initial
            text_to_parse = re.sub(r"```json\s*", "", final_response_text)
            text_to_parse = re.sub(r"```$", "", text_to_parse.strip())

            # Initialisation de la boucle avec le premier r√©sultat
            current_tool_result = self._detecter_et_executer_function_call(
                text_to_parse
            )

            # Limite de s√©curit√© pour √©viter les boucles infinies
            max_autonomy_steps = 10
            step_count = 0

            # 2. D√©marrage de la Machine √† √âtats
            while current_tool_result and step_count < max_autonomy_steps:
                step_count += 1
                prompt_autonome_obj = None

                # --- A. ROUTAGE STRICT SELON LE R√âSULTAT ---

                # CAS 0 : SORTIE DIRECTE (Final Answer)
                if current_tool_result.get("type") == "FINAL_ANSWER_EXTRACTED":
                    self.logger.info("üèÅ SORTIE BOUCLE : R√©ponse Finale")
                    contenu_final = current_tool_result.get("content", "")
                    if stream and is_hidden_json_mode:
                        yield contenu_final
                    final_response_text = contenu_final
                    break

                # CAS 1 : R√âSULTAT M√âMOIRE (Carte ou Fichier)
                elif current_tool_result.get("type") == "MEMORY_RESULTS":
                    payload = current_tool_result["payload"]
                    item = payload[0] if isinstance(payload, list) and payload else None

                    if item and item.type == "cartographie_projet":
                        self.logger.info("üó∫Ô∏è √âTAT: NAVIGATION (CartographyPrompt)")
                        prompt_autonome_obj = CartographyPrompt(
                            prompt_original=prompt,
                            instructions_cartographie=self.agent_parole.recuperer_instruction(
                                "instructions_cartographie"
                            ),
                            cartographie_projet=item.contenu,
                            plan_de_bataille=[
                                self.agent_parole._recuperer_resume_systeme()
                            ],
                            intention=resultat_intention,
                        )

                    elif item and item.type in ["fichier_technique", "fichier_brut"]:
                        self.logger.info(
                            f"üîß √âTAT: INSPECTION (FileInspectionPrompt) - {item.titre}"
                        )
                        prompt_autonome_obj = FileInspectionPrompt(
                            prompt_original=prompt,
                            instructions_inspection=self.agent_parole.recuperer_instruction(
                                "instructions_inspection"
                            ),
                            fichier_en_cours=item,
                            notes_precedentes=self.agent_parole._recuperer_resume_systeme(),
                            intention=resultat_intention,
                        )

                    else:
                        if step_count == 1:
                            self.logger.info(
                                "üöÄ √âTAT: STRAT√âGIE INITIALE (MemorySearchFirstPrompt)"
                            )
                            prompt_autonome_obj = MemorySearchFirstPrompt(
                                prompt_original=prompt,
                                instructions_first_search=self.agent_parole.recuperer_instruction(
                                    "instructions_memory_search_first_prompt"
                                ),
                                resultats_memoire=payload,
                                intention=resultat_intention,
                            )
                        else:
                            self.logger.info(
                                "üîç √âTAT: ENQU√äTE CONTINUE (MemorySearchPrompt)"
                            )
                            prompt_autonome_obj = MemorySearchPrompt(
                                prompt_original=prompt,
                                instructions_memory_search_prompt=self.agent_parole.recuperer_instruction(
                                    "instructions_memory_search_prompt"
                                ),
                                resultats_memoire=payload,
                                raisonnement_precedent=self.active_plan,
                                intention=resultat_intention,
                            )

                # CAS 2 : APR√àS MODIFICATION (Staging Review)
                elif current_tool_result.get("function") == "update_system_summary":
                    self.logger.info("‚úÖ √âTAT: REVIEW (StagingReviewPrompt)")
                    prompt_autonome_obj = StagingReviewPrompt(
                        prompt_original=prompt,
                        instructions_review=self.agent_parole.recuperer_instruction(
                            "instructions_review"
                        ),
                        etat_staging_actuel=self.agent_parole._recuperer_resume_systeme(),
                        derniere_action=str(
                            current_tool_result.get("results", "Mise √† jour effectu√©e")
                        ),
                        intention=resultat_intention,
                    )

                # CAS 3 : R√âSULTAT G√âN√âRIQUE
                elif "results" in current_tool_result:
                    prompt_autonome_obj = MemorySearchPrompt(
                        prompt_original=prompt,
                        instructions_memory_search_prompt=self.agent_parole.recuperer_instruction(
                            "instructions_memory_search_prompt"
                        ),
                        resultats_memoire=[
                            Souvenir(
                                contenu=str(current_tool_result["results"]),
                                type="tool_result",
                                titre="Resultat Outil",
                                score=1.0,
                            )
                        ],
                        raisonnement_precedent=self.active_plan,
                        intention=resultat_intention,
                    )

                # --- B. G√âN√âRATION DE LA R√âPONSE INTERM√âDIAIRE ---
                # (Ce bloc IF doit √™tre align√© verticalement avec les ELIF ci-dessus)
                if prompt_autonome_obj:
                    # 1. Construction
                    prompt_txt = self.agent_parole.construire_prompt_llm(
                        prompt_autonome_obj
                    )

                    # 2. G√©n√©ration
                    reponse_interne = ""
                    for token in self.moteur_llm.generer_stream(prompt_txt):
                        reponse_interne += token
                        if stream:
                            yield token

                    # 3. Ex√©cution
                    text_interne_clean = re.sub(r"```json\s*", "", reponse_interne)
                    text_interne_clean = re.sub(r"```$", "", text_interne_clean.strip())

                    next_tool = self._detecter_et_executer_function_call(
                        text_interne_clean
                    )

                    if next_tool:
                        current_tool_result = next_tool
                    else:
                        final_response_text = reponse_interne
                        break
                else:
                    break
        # ==========================================================
        # 10. Post-Traitement Asynchrone (Sauvegarde & Stats)
        # ==========================================================
        self.agent_contexte.mettre_a_jour_historique(prompt, final_response_text)

        if llm_success:
            try:
                interaction_brute = Interaction(
                    prompt=prompt,
                    reponse=final_response_text,
                    system=self.agent_parole.recuperer_instruction(
                        "instructions_systeme"
                    ),
                    intention=prompt_final_obj.intention,
                    contexte_memoire=[],
                    meta=MetadataFichier(
                        id=interaction_id, session_id=session_id, type_memoire="brute"
                    ),
                )
                self.agent_memoire.sauvegarder_interaction_brute(interaction_brute)
            except Exception as e:
                self.logger.log_error(f"Erreur sauvegarde brute : {e}")

            self.current_message_turn += 1
            self.derniere_interaction = (prompt, final_response_text, datetime.now())

            try:
                threading.Thread(
                    target=lambda: self.post_traitement_async(
                        prompt,
                        final_response_text,
                        prompt_final_obj,
                        meta_pipeline.interaction_id,
                        self.current_session_id,
                        self.current_message_turn,
                    ),
                    daemon=True,
                ).start()
            except Exception as e:
                self.logger.log_error(
                    f"Erreur thread post-traitement: {e}", exc_info=True
                )

        if not stream:
            yield final_response_text

    def _gerer_commandes_systeme(self, prompt: str, stream: bool) -> Optional[Dict]:
        """
        [ATOME] G√®re les commandes syst√®me (+1, -1) et les protocoles (!!!).
        Si une action coupe le flux normal, retourne imm√©diatement le r√©sultat (yield).
        """
        prompt_clean = prompt.strip()

        # ------üö® Logique du Protocole ALERTE (!!!)------------
        # - Priorit√© 1
        # ------------------------------------------------------
        # CORRECTION : On s'assure que "!!!" est explicite et pas juste une partie de code
        # On demande que "!!!" soit au d√©but ou pr√©c√©d√© d'un espace, ou seul sur la ligne
        is_alert = "!!!" in prompt_clean and not prompt_clean.startswith(
            "#!"
        )  # √âvite le shebang

        if is_alert:
            self.logger.signal_gouvernance(
                "SIGNAL '!!!' D√âTECT√â. Activation Protocole ALERTE."
            )

            # 1. Lancement analyse r√©flexive en fond (On garde √ßa pour les stats/logs)
            try:
                import threading

                threading.Thread(
                    target=lambda: self.agent_reflexor.lancer_analyse_gouvernance(
                        prompt_erreur=prompt,
                        historique=self.agent_contexte.get_historique_chat(),
                    ),
                    daemon=True,
                ).start()
            except Exception:
                pass

            # 2. Construction de l'objet ProtocolePrompt
            # On r√©cup√®re les ingr√©dients via Contexte
            contenu_protocole = self.agent_contexte.recuperer_protocole_alerte()
            historique_court = self.agent_contexte.get_historique_recent(limit=10)

            # On force une intention de crise
            intention_forcee = ResultatIntention(
                prompt=prompt,
                sujet=Sujet.SECONDMIND,
                action=Action.DEBUG,
                categorie=Categorie.SYSTEME,
            )

            req_protocole = ProtocolePrompt(
                prompt_original=prompt,
                protocole_contenu=contenu_protocole,
                historique_recent=historique_court,
                intention=intention_forcee,
                regles=[],  # Pas de r√®gles standard, le protocole est la loi
            )

            # On retourne un signal sp√©cial pour 'penser'
            return {"type": "PROTOCOLE_ALERTE", "payload": req_protocole}
        # ------------------------------------------------------
        # DETECTION PREMIER PROMPT
        # ------------------------------------------------------

        p = prompt.lower().strip()

        # D√©clencheur conversationnel (STRICT)
        # On ne d√©clenche le "Lite Mode" que si le message est court (< 10 mots)
        # et contient une salutation. Sinon, on passe par le pipeline complet.
        salutations = ["salut"]
        if any(s == p or (s in p and len(p.split()) < 10) for s in salutations):
            return "NOUVEAU_CHAT"

        # Initialisation par d√©faut pour √©viter UnboundLocalError
        keyword = None
        # ------------------------------------------------------
        # üöÄ Commandes Syst√®me (+1, -1)
        # ------------------------------------------------------

        if prompt_clean.startswith("+1") or prompt_clean.startswith("-1"):
            score = 1.0 if prompt_clean.startswith("+1") else 0.0
            mots = prompt_clean.split()
            keyword = mots[1].lower() if len(mots) > 1 else "g√©n√©ral"

            # V√©rifier qu'on a bien une interaction pr√©c√©dente
            if not (self.derniere_interaction and len(self.derniere_interaction) >= 2):
                return {"response": "Aucune interaction r√©cente √† √©valuer."}

            original_prompt, final_response_text, *_ = self.derniere_interaction

            # Lancer l'enregistrement dans un thread (pas de lambda capturant l'environnement)
            try:
                import threading
                from functools import partial

                target_fn = partial(
                    self.agent_reflexor.enregistrer_feedback_etendu,
                    prompt=original_prompt,
                    reponse=final_response_text,
                    score=score,
                    mot_cle=keyword,
                )
                threading.Thread(target=target_fn, daemon=True).start()
                self.logger.info(
                    "‚úÖ Enregistrement feedback lanc√© dans un thread asynchrone."
                )
            except Exception as e:
                self.logger.log_error(f"√âchec du lancement du thread de feedback: {e}")
                return {
                    "response": "Erreur interne lors de l'enregistrement du feedback."
                }

            # R√©ponses synchrones rapides selon mot-cl√©
            if keyword == "m√©moire":
                return {
                    "response": f"üß† Feedback M√©moire ({'+1' if score else '-1'}) enregistr√©. Index Whoosh mis √† jour."
                }

            if keyword == "pertinence":
                # logique existante de sauvegarde (garde comportement synchrone)
                try:
                    original_prompt, final_response_text, *_ = self.derniere_interaction
                    feedback_data = {
                        "timestamp": datetime.now().isoformat(),
                        "type": "feedback_pertinence_juge",
                        "score_utilisateur": score,
                        "context": {
                            "prompt": original_prompt,
                            "reponse": final_response_text,
                            "classification_precedente": asdict(
                                self.derniere_classification
                            )
                            if self.derniere_classification
                            else None,
                        },
                        "commentaire": "Valid√© par commande vocale (+1 pertinence)"
                        if score > 0.5
                        else "Invalid√© par commande vocale (-1 pertinence)",
                    }
                    status = "ok" if score > 0.5 else "bad"
                    nom_fichier = f"feedback_pertinence/juge_{status}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    self.agent_memoire.sauvegarder_memoire(
                        contenu=feedback_data,
                        type_memoire="reflexive",
                        nom_fichier=nom_fichier,
                    )
                    return {
                        "response": f"üéØ Feedback Pertinence ({'+1' if score > 0.5 else '-1'}) enregistr√©. Dataset Juge mis √† jour."
                    }
                except Exception as e:
                    self.logger.log_error(f"Erreur sauvegarde feedback pertinence: {e}")
                    return {"response": "Erreur lors de l'enregistrement du feedback."}

            return {
                "response": "‚úÖ Feedback enregistr√© et traitement asynchrone lanc√©."
            }
        # -----------------------------------------------------
        # üöÄ Nouvelle Commande : Feedback Intention (-1 intention)
        if not stream and prompt_clean.startswith("-1 intention"):
            # V√©rifier si l'interaction pr√©c√©dente et la classification sont disponibles
            if self.derniere_interaction and self.derniere_classification:
                # 1. Obtenir le chemin de sauvegarde (Assumant 'feedback' dans l'Auditor)
                feedback_dir_path = self.auditor.get_path(
                    "feedback", nom_agent="memoire"
                )
                if not feedback_dir_path:
                    self.logger.log_error(
                        "‚ùå Chemin feedback introuvable dans Auditor."
                    )
                    return {"response": "Erreur: Chemin de feedback introuvable."}

                # 2. Pr√©parer le contenu
                # On utilise l'objet ResultatIntention directement pour le JSON
                original_prompt, _, _ = self.derniere_interaction

                feedback_data = {
                    "timestamp": datetime.now().isoformat(),
                    "prompt_critique": original_prompt,
                    "classification_predite": asdict(
                        self.derniere_classification
                    ),  # ‚úÖ CORRIG√â
                    "commentaire": "Intention d√©tect√©e comme incorrecte par l'utilisateur (-1 intention)",
                }

                # 3. √âcrire le fichier JSON horodat√© (dans le dossier reflexive/feedback)
                dossier = Path(feedback_dir_path)
                dossier.mkdir(parents=True, exist_ok=True)  # Assurer l'existence

                nom_fichier = f"feedback_intention_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                chemin_fichier = dossier / nom_fichier

                try:
                    # Utiliser l'encodeur standardis√© pour g√©rer les Enums et Dataclasses
                    with open(chemin_fichier, "w", encoding="utf-8") as f:
                        # CustomJSONEncoder est dans contrats_interface.py
                        json.dump(
                            feedback_data,
                            f,
                            ensure_ascii=False,
                            indent=2,
                            cls=CustomJSONEncoder,
                        )

                    self.logger.signal_gouvernance(
                        f"‚úÖ Feedback Intention (-1) enregistr√©: {nom_fichier}"
                    )
                    return {
                        "response": "üéØ Intention de la derni√®re requ√™te enregistr√©e comme incorrecte. Analyse √† faire."
                    }

                except Exception as e:
                    self.logger.log_error(
                        f"‚ùå Erreur sauvegarde feedback intention: {e}"
                    )
                    return {
                        "response": "Erreur lors de la sauvegarde du feedback intention."
                    }

            return {
                "response": "Aucune interaction r√©cente avec classification √† √©valuer."
            }

        return None

    def _handle_forced_search(self, prompt: str, search_mode: str) -> Optional[Any]:
        """
        G√®re le mode de recherche forc√©e (Web uniquement).
        Redirige vers le module Deep Research de l'AgentRecherche.
        """
        if search_mode == "web":
            query = (
                prompt.replace("recherche_web", "").replace("recherche web", "").strip()
            )
            self.logger.info(f"üîç Deep Research forc√©e sur : '{query}'")

            # Appel √† l'Agent Recherche (qui utilise l'outil RechercheWeb)
            # Cela peut prendre du temps (boucle Search -> Read -> Evaluate)
            rapport_final = self.agent_recherche.recherche_web_profonde(query)

            # On renvoie le rapport complet comme r√©ponse unique
            # (Note : Ce n'est pas stream√© token par token, c'est un bloc de texte)
            yield rapport_final
            return

        return None

    def post_traitement_async(
        self,
        prompt: str,
        reponse: str,
        standard_prompt: StandardPrompt,
        interaction_id: str,
        session_id: str,
        message_turn: int,
    ):
        """
        T√¢ches de fond (Fire-and-Forget).

        Ex√©cut√© dans un Thread Daemon pour ne pas bloquer la r√©ponse utilisateur (UI Latency).
        Responsabilit√©s :
        1. **Code Extraction** : Parsing de la r√©ponse pour extraire/sauvegarder les snippets (.py).
        2. **Sanitization** : Nettoyage des donn√©es (retrait du contenu brut des fichiers) avant stockage.
        3. **Persistance** : √âcriture du log JSON final (Interaction) via AgentMemoire.
        4. **Juge** : √âvaluation asynchrone de la qualit√© de la r√©ponse (si activ√©).
        """
        try:
            reponse_pour_historique = reponse
            # ===========================================================
            # 1. EXTRACTION & TRAITEMENT DU CODE (Nouveau Pipeline)
            # ===========================================================
            if getattr(self, "agent_code", None):
                try:
                    # On demande √† l'AgentCode de s√©parer le texte du code
                    texte_nettoye_api, artefacts = (
                        self.agent_code.extractor_manager.traiter_reponse_llm(reponse)
                    )

                    # A. Sauvegarde des fichiers physiques (si code d√©tect√©)
                    if artefacts:
                        self.agent_memoire.sauvegarder_artefacts_code(artefacts)
                        self.dernier_code_hash = artefacts[-1]["hash"]

                    # B. On r√©cup√®re le texte nettoy√© par l'API (s'il existe, sinon on garde l'original)
                    if texte_nettoye_api:
                        reponse_pour_historique = texte_nettoye_api

                except Exception as e:
                    self.logger.log_error(f"Erreur extraction code: {e}")

            # ===========================================================
            # 1-BIS. NETTOYAGE ULTIME (S√©curit√© Regex)
            # ===========================================================
            # M√™me si l'AgentCode a rat√© son coup, on FORCE le retrait visuel des blocs de code
            # pour ne pas polluer le JSON historique avec des milliers de lignes de code.
            import re

            pattern_code = r"```[\s\S]*?```"
            if re.search(pattern_code, reponse_pour_historique):
                reponse_pour_historique = re.sub(
                    pattern_code,
                    "\n\n[... üíæ CODE EXTRAIT ET SAUVEGARD√â DANS /memoire/code/ ...]\n\n",
                    reponse_pour_historique,
                )

            # ===========================================================
            # 2. PR√âPARATION S√âCURIS√âE DES DONN√âES (Fix du Crash)
            # ===========================================================
            # On utilise des listes vides par d√©faut si standard_prompt est None
            souvenirs = getattr(standard_prompt, "souvenirs", []) or []
            # B. Le Contexte Technique/L√©gislatif -> Pour 'meta.data_libre'
            # On utilise asdict pour s√©rialiser proprement en JSON
            regles_objs = getattr(standard_prompt, "regles", []) or []
            docs_objs = getattr(standard_prompt, "fichiers_readme", []) or []
            code_objs = (
                getattr(
                    standard_prompt,
                    "code_chunks",
                    getattr(standard_prompt, "contexte_code", []),
                )
                or []
            )

            # Conversion en dict pour stockage JSON dans data_libre
            regles_data = [asdict(r) for r in regles_objs]
            docs_data = [asdict(d) for d in docs_objs]
            # Pour le code, on g√®re le cas o√π ce n'est pas une dataclass pure (parfois dict)
            code_data = []
            for c in code_objs:
                if is_dataclass(c):
                    code_data.append(asdict(c))
                elif isinstance(c, dict):
                    code_data.append(c)
                else:
                    code_data.append({"contenu": str(c)})
            # ===========================================================
            # 3. JUGE (Si actif)
            # ===========================================================
            valide_juge = True
            score_juge = 1.0
            raison_juge = "Pas de juge actif"

            if getattr(self, "agent_juge", None):
                try:
                    contexte_str = "\n".join([s.contenu for s in souvenirs + docs_objs])
                    res_juge = self.agent_juge.evaluer_coherence_reponse(
                        contexte_rag_str=contexte_str,
                        prompt=prompt,
                        reponse=reponse_pour_historique,
                    )
                    valide_juge = res_juge.valide
                    score_juge = res_juge.score
                    raison_juge = res_juge.raison
                except Exception:
                    pass
            # ===========================================================
            # 4. G√âN√âRATION DU R√âSUM√â (MiniLLM)
            # ===========================================================
            resume_interaction = "√âchange standard."
            if getattr(self, "moteur_mini_llm", None):
                try:
                    p_resume = f"R√©sum√© 1 phrase:\nUser: {prompt[:300]}\nAssistant: {reponse_pour_historique[:300]}"
                    # On consomme le g√©n√©rateur
                    resume_interaction = "".join(
                        list(self.moteur_mini_llm.generer_stream(p_resume))
                    )
                except Exception:
                    pass

            # ===========================================================
            # üõ°Ô∏è PURGE M√âMOIRE
            # On ne veut PAS sauvegarder le contenu des fichiers lus dans l'historique.
            # On garde la r√©f√©rence (titre/chemin) mais on vide le contenu texte.
            # ===========================================================
            souvenirs_nettoyes = []
            for s in souvenirs:
                # Si c'est un fichier technique ou du code, on vide le contenu
                if s.type in [
                    "fichier_technique",
                    "fichier_brut",
                    "code",
                    "fichier_actif",
                ]:
                    # On cr√©e une COPIE pour ne pas casser l'affichage imm√©diat si n√©cessaire
                    from dataclasses import replace

                    s_clean = replace(
                        s,
                        contenu=f"[Fichier '{s.titre}' consult√© - Contenu non persist√© dans l'historique]",
                    )
                    souvenirs_nettoyes.append(s_clean)
                else:
                    souvenirs_nettoyes.append(s)

            # On fait pareil pour les chunks de code
            code_objs_nettoyes = []
            for c in code_objs:
                # Si c'est une Dataclass, on remplace
                if is_dataclass(c):
                    from dataclasses import replace

                    # On garde juste la signature/nom
                    nom = getattr(c, "chemin", getattr(c, "name", "Inconnu"))
                    c_clean = replace(
                        c, contenu=f"[Snippet '{nom}' utilis√© - Non persist√©]"
                    )
                    code_objs_nettoyes.append(c_clean)
                else:
                    code_objs_nettoyes.append(
                        c
                    )  # Cas dictionnaire, on laisse (ou on nettoie si besoin)

            # ===========================================================
            # 5. CONSTRUCTION M√âTADONN√âES & SAUVEGARDE
            # ===========================================================

            # Agr√©gation des noms de fichiers pour la tra√ßabilit√© rapide
            fichiers_vus = [s.titre for s in souvenirs]
            fichiers_vus.extend([f"REGLE:{r.titre}" for r in regles_objs])
            fichiers_vus.extend([f"DOC:{d.titre}" for d in docs_objs])
            for c in code_objs:
                nom = getattr(c, "chemin", getattr(c, "name", "Snippet"))
                fichiers_vus.append(f"CODE:{nom}")

            meta_fichier = MetadataFichier(
                id=interaction_id,
                session_id=session_id,
                message_turn=message_turn,
                source_agent="Semi",
                fichiers_consultes=fichiers_vus,
                validation_juge=valide_juge,
                score_qualite=score_juge,
                details_juge=raison_juge,
                len_contenu=len(reponse_pour_historique),
                # ‚úÖ STOCKAGE S√âPAR√â : On garde la trace sans polluer les souvenirs
                data_libre={
                    "resume_semantique": resume_interaction,
                    "contexte_technique": "RAG Code" if code_objs else "Standard",
                    "snapshot_regles": regles_data,
                    "snapshot_fichiers_readme": docs_data,
                    "snapshot_code": code_data,
                },
            )

            # Cr√©ation de l'objet Interaction avec typage STRICT
            interaction_obj = Interaction(
                prompt=prompt,
                reponse=reponse_pour_historique,
                system="Instructions Syst√®me",
                intention=getattr(standard_prompt, "intention", None),
                # ‚úÖ CORRECTIF FINAL : Uniquement des Souvenirs ici
                contexte_memoire=souvenirs_nettoyes,
                meta=meta_fichier,
            )

            # üõ°Ô∏èüëÅÔ∏è‚Äçüó®Ô∏èüõ°Ô∏è# VALIDATION FORMAT SORTIE
            # On v√©rifie l'int√©grit√© avant d'√©crire sur le disque
            self.auditor.valider_format_sortie(interaction_obj)

            self.agent_memoire.memoriser_interaction(interaction_obj)

        except Exception as e:
            self.logger.log_error(
                f"‚ùå Erreur CRITIQUE post-traitement: {e}", exc_info=True
            )

    # M√âCANIQUE DE BATCH DE VECTORISATION PERSISTANTE AU D√âMARRAGE
    # ===========================================================
    def _verifier_batch_au_demarrage(self):
        """
        V√©rifie si le traitement batch diff√©r√© doit √™tre lanc√© au d√©marrage.
        Se d√©clenche si le dernier run date de plus de 45h ou n'existe pas.
        """
        try:
            state = self.processeur_batch._charger_etat()
            dernier_run_str = state.get("dernier_run")

            # CAS 1 : Premier lancement ou √©tat perdu
            if not dernier_run_str:
                self.logger.info(
                    "üïí Aucun batch pr√©c√©dent d√©tect√©. Lancement asynchrone imm√©diat."
                )
                # ‚úÖ CORRECTION : Lancement dans un thread pour ne pas bloquer le d√©marrage du serveur
                self._lancer_batch_async()
                return

            # CAS 2 : V√©rification du d√©lai
            dernier_run = datetime.fromisoformat(dernier_run_str)
            delta = datetime.now() - dernier_run
            heures_ecoulees = delta.total_seconds() / 3600

            if heures_ecoulees >= 45:
                self.logger.info(
                    f"üïí Batch obsol√®te ({heures_ecoulees:.1f}h). Lancement asynchrone..."
                )
                self._lancer_batch_async()
            else:
                self.logger.info(
                    f"‚úÖ Batch r√©cent ({heures_ecoulees:.1f}h). Aucune action."
                )

        except Exception as e:
            self.logger.log_error(f"Erreur v√©rification batch: {e}")

    def _lancer_batch_async(self):
        """
        Lance le traitement batch (consolidation m√©moire) dans un thread s√©par√©.
        Appel√© par _verifier_batch_au_demarrage.
        """
        try:
            import threading

            threading.Thread(
                target=self.processeur_batch.traiter_batch_differe, daemon=True
            ).start()
            self.logger.info("üöÄ Batch de vectorisation lanc√© en arri√®re-plan.")
        except Exception as e:
            self.logger.log_error(f"√âchec lancement batch async: {e}")

    # ----------------------------------------------------------------
    # CAPACIT√âS COGNITIVES (RAG ARCHITECTURE & CODE)
    # ----------------------------------------------------------------

    def consulter_architecture_et_code(self, question: str) -> str:
        """
        Outil critique : √Ä utiliser d√®s que tu dois r√©pondre √† une question sur
        le fonctionnement interne, le code, les fichiers ou l'architecture du projet.
        Permet de lire le code source r√©el du projet.

        Args:
            question (str): La question technique pr√©cise (ex: "Comment fonctionne agent_Memoire ?")

        Returns:
            str: Un r√©sum√© contextuel contenant les bouts de code pertinents.
        """
        if not self.agent_code:
            return "Indisponible : Le module RAG Code n'est pas initialis√©."

        self.logger.log_info(f"üèóÔ∏è Je consulte l'architecture code pour : {question}")

        # On d√©l√®gue √† l'adapter qui g√®re la complexit√© (vecteurs + graphe)
        contexte = self.agent_code.fournir_contexte(question)

        return contexte

    def _exec_update_system_summary(self, new_content: str) -> str:
        """
        Met √† jour etat_systeme_resume.md dans le dossier de l'agent.
        Mode: APPEND (Ajout) pour conserver l'historique des modifications.
        """
        try:
            from datetime import datetime
            from pathlib import Path

            # 1. Localisation (Dossier Semi)
            path_semi_dir = self.auditor.get_path("agent_dir")
            # Fallback si l'auditor ne r√©pond pas (ex: test unitaire)
            if not path_semi_dir:
                path_semi_dir = Path(__file__).parent

            f_dest = Path(path_semi_dir) / "etat_systeme_resume.md"

            # 2. Pr√©paration de l'horodatage
            now_str = datetime.now().strftime("%Y-%m-%d %H:%M")

            # Si le fichier n'existe pas encore, on le cr√©e avec un en-t√™te
            if not f_dest.exists():
                with open(f_dest, "w", encoding="utf-8") as f:
                    f.write(f"# üß† √âTAT SYST√àME & R√âSUM√â\n_Initialis√© le {now_str}_\n")

            # 3. Pr√©paration du bloc √† ajouter
            # On ajoute un saut de ligne propre avant le nouveau bloc
            bloc_ajout = f"\n\n### üìÖ Mise √† jour du {now_str}\n{new_content}\n---"

            # 4. √âcriture en mode AJOUT ('a')
            with open(f_dest, "a", encoding="utf-8") as f:
                f.write(bloc_ajout)

            self.logger.info(f"üìù R√©sum√© syst√®me mis √† jour (Ajout) : {f_dest.name}")
            return f"Succ√®s : Information ajout√©e √† {f_dest.name}."

        except Exception as e:
            self.logger.log_error(f"Erreur √©criture r√©sum√© : {e}")
            return f"Erreur critique : {e}"

    def _extraire_bloc_json(self, texte: str) -> str:
        """
        Extrait le premier bloc JSON valide en comptant les accolades imbriqu√©es,
        tout en ignorant celles contenues dans les cha√Ænes de caract√®res.
        """
        texte = texte.strip()
        idx_debut = texte.find("{")
        if idx_debut == -1:
            return ""

        compteur = 0
        in_string = False
        escape = False

        for i, char in enumerate(texte[idx_debut:], start=idx_debut):
            # 1. Gestion de l'√©tat "Dans une cha√Æne de caract√®res"
            if char == '"' and not escape:
                in_string = not in_string

            # 2. Gestion de l'√©chappement (ex: \" √† l'int√©rieur d'une cha√Æne)
            if char == "\\" and not escape:
                escape = True
                continue  # On saute le backslash pour ne pas le traiter deux fois

            escape = False  # Reset de l'√©chappement pour le caract√®re suivant

            # 3. Comptage des accolades (UNIQUEMENT si on n'est pas dans une string)
            if not in_string:
                if char == "{":
                    compteur += 1
                elif char == "}":
                    compteur -= 1
                    # Quand on retombe √† 0, c'est la fin du JSON
                    if compteur == 0:
                        return texte[idx_debut : i + 1]

        return ""

    # =========================================================================
    # üîß GESTION DES OUTILS (VERSION ROBUSTE RESTAUR√âE)
    # =========================================================================

    def _detecter_et_executer_function_call(self, response: str) -> Optional[Dict]:
        """
        Parseur et Dispatcheur d'Outils (Function Calling local).

        Analyse la r√©ponse textuelle pour extraire un bloc JSON valide (gestion des accolades imbriqu√©es).
        Mappe le champ "function" vers les m√©thodes internes :
        - `recherche_web` -> AgentRecherche (Deep Search)
        - `rechercher_memoire` -> RechercheMemoireTool (RAG/Files)
        - `update_system_summary` -> Mise √† jour du fichier d'√©tat global.

        Returns:
            Dict: Le r√©sultat de l'ex√©cution de l'outil (souvent inject√© dans le prompt suivant).
        """
        import json
        import re

        # 1. Extraction ROBUSTE (R√©cup√©ration de ta m√©thode logicielle)
        json_str = self._extraire_bloc_json(response)
        if not json_str:
            return None

        # 2. Nettoyage
        json_str = json_str.replace("```json", "").replace("```", "").strip()

        # 3. Parsing & R√©paration (Chemins Windows - CRITIQUE & CORRIG√â)
        try:
            # üõë CORRECTION MAJEURE : Negative Lookbehind (?<!\\)
            # Cette regex dit : "Remplace le \ par \\ SEULEMENT S'IL N'EST PAS D√âJ√Ä PR√âC√âD√â d'un \"
            # Ainsi : "D:\Dev" devient "D:\\Dev" (Fix√©)
            # Mais :  "D:\\Dev" reste "D:\\Dev" (Pas touch√©)
            json_str_fixed = re.sub(r'(?<!\\)\\(?![/u"\\bfnrt])', r"\\\\", json_str)

            function_call = json.loads(json_str_fixed)

        except json.JSONDecodeError:
            # Fallback ultime : Si le JSON est vraiment cass√©, on tente le mode permissif "strict=False"
            try:
                function_call = json.loads(json_str, strict=False)
            except Exception:
                self.logger.log_warning(f"√âchec parsing JSON final: {json_str[:50]}...")
                return None

        # 4. Capture du PLAN (State Passing - CRITIQUE POUR AUTONOMIE)
        if "plan_update" in function_call:
            self.active_plan = function_call["plan_update"]
            self.logger.info(f"üìÖ Plan mis √† jour : {len(self.active_plan)} √©tapes.")

        # Gestion de la structure imbriqu√©e (next_action) ou plate
        action_data = function_call.get("next_action", function_call)

        # CAS 0 : R√âPONSE FINALE
        if isinstance(action_data, dict) and action_data.get("type") == "final_answer":
            return {
                "type": "FINAL_ANSWER_EXTRACTED",
                "content": action_data.get("content", ""),
            }

        function_name = action_data.get("function")
        arguments = action_data.get("arguments", {})
        if not function_name:
            return None

        # 5. EX√âCUTION (Restauration int√©grale des outils + Ajouts)
        try:
            self.logger.info(f"‚öôÔ∏è Tentative ex√©cution outil : {function_name}")

            # --- OUTIL : RECHERCHE WEB ---
            if function_name == "recherche_web":
                query = arguments.get("query", "")
                # On utilise l'agent recherche s'il a la capacit√©, sinon fallback
                if hasattr(self.agent_recherche, "recherche_web_profonde"):
                    rapport = self.agent_recherche.recherche_web_profonde(query)
                    return {"function": function_name, "results": rapport}
                return {
                    "function": function_name,
                    "results": "Recherche web non disponible.",
                }

            # --- OUTIL : M√âMOIRE (D√©l√©gu√© √† RechercheMemoireTool) ---
            elif function_name == "rechercher_memoire":
                if not hasattr(self.agent_recherche, "outil_recherche_memoire"):
                    return {
                        "function": function_name,
                        "results": "Erreur: Outil m√©moire non charg√©.",
                    }

                # Support cach√© pour lire_cartographie via query
                q = arguments.get("query", "")
                if "cartographie" in q.lower() or "project_map" in q.lower():
                    return self.agent_recherche.outil_recherche_memoire.traiter_lecture_cartographie(
                        {}
                    )

                return self.agent_recherche.outil_recherche_memoire.traiter_recherche_memoire(
                    arguments
                )

            # --- OUTIL : CARTOGRAPHIE (Nouveau Standard) ---
            elif function_name == "lire_cartographie":
                if not hasattr(self.agent_recherche, "outil_recherche_memoire"):
                    return {
                        "function": function_name,
                        "results": "Erreur: Outil m√©moire non charg√©.",
                    }
                return self.agent_recherche.outil_recherche_memoire.traiter_lecture_cartographie(
                    {}
                )

            # --- OUTIL : SYST√àME ---
            elif function_name == "update_system_summary":
                content = arguments.get("content", "")
                # CORRECTION : Appel de la m√©thode qui accepte (new_content)
                result = self._exec_update_system_summary(content)
                return {"function": function_name, "results": result}

            # --- OUTIL : FINAL ANSWER (Explicite) ---
            elif function_name == "final_answer":
                return {
                    "type": "FINAL_ANSWER_EXTRACTED",
                    "content": arguments.get("content", "Termin√©."),
                }

            else:
                return {
                    "function": function_name,
                    "results": f"Outil inconnu : {function_name}",
                }

        except Exception as e:
            self.logger.log_error(f"Erreur ex√©cution tool {function_name}: {e}")
            return {"function": function_name, "results": f"Exception technique : {e}"}

    # =================================================================
    # üß† PROPRIOCEPTION : G√âN√âRATION DU R√âSUM√â SYST√àME (Ancrage Root)
    # =================================================================
    def actualiser_resume_systeme(self):
        """
        Proprioception : Met √† jour le fichier 'etat_systeme_resume.md'.

        Lit les fichiers de logs (historique, todo) pour construire une "conscience de soi"
        √† jour, qui sera inject√©e dans le System Prompt de la prochaine requ√™te.
        Assure que l'agent sait ce qu'il a fait la veille.
        """
        try:
            self.logger.info("üß† Mise √† jour de la Conscience du Syst√®me...")

            # 1. R√©cup√©ration du dossier via l'Auditor
            # On demande le chemin 'agent_dir' configur√© pour 'semi'
            dossier_semi = Path(self.auditor.get_path("agent_dir"))

            if not dossier_semi.exists():
                self.logger.log_error(
                    f"Dossier Semi introuvable via Auditor : {dossier_semi}"
                )
                return False

            f_hist = dossier_semi / "historique_secondmind.md"
            f_todo = dossier_semi / "todo_secondmind.md"
            f_dest = dossier_semi / "etat_systeme_resume.md"

            # 2. Cr√©ation si absent (Initialisation)
            if not f_hist.exists():
                f_hist.write_text(
                    "# Historique SecondMind\n\n## Initialisation", encoding="utf-8"
                )
            if not f_todo.exists():
                f_todo.write_text(
                    "# TODO SecondMind\n\n## Priorit√© Haute", encoding="utf-8"
                )

            # 3. Lecture Simple (5 premi√®res lignes)
            # (On garde votre logique simplifi√©e sans LLM pour l'instant, comme demand√© pr√©c√©demment)
            def get_top_5_lines(path):
                try:
                    text = path.read_text(encoding="utf-8")
                    lines = [line.strip() for line in text.splitlines() if line.strip()]
                    return lines[:5]
                except Exception as e:
                    return [f"Erreur lecture: {e}"]

            hist_lines = get_top_5_lines(f_hist)
            todo_lines = get_top_5_lines(f_todo)

            # 4. Assemblage
            contenu_final = f"""# R√©sum√© de l‚Äô√©tat du syst√®me
_Mise √† jour : {datetime.now().strftime("%Y-%m-%d %H:%M")}_

## üìú Historique (R√©cent)

"""
            for ligne in hist_lines:
                contenu_final += f"- {ligne}\n"

            contenu_final += "\n## üìù √Ä Faire (Prioritaire)\n"
            for ligne in todo_lines:
                contenu_final += f"- {ligne}\n"

            contenu_final += "\n## üìä √âtat Global\nSyst√®me actif."

            # 5. √âcriture
            f_dest.write_text(contenu_final, encoding="utf-8")
            self.logger.info(f"‚úÖ R√©sum√© mis √† jour dans : {f_dest}")
            return True

        except Exception as e:
            self.logger.log_error(f"Erreur Proprioception : {e}")
            return False

    ##############################################
    # M√©thode pour g√©rer le contexte du frontend #
    ##############################################
    def _format_manual_context(self, manual_context: List[Dict]) -> str:
        """Formate le contexte manuel des slots en string pour agent_Parole"""
        if not manual_context:
            return None

        formatted = ""
        for slot in manual_context:
            title = slot.get("title", "Slot")
            content = slot.get("content", "")
            formatted += f"\n### {title}\n{content}\n"

        self.logger.log_thought(f"Contexte manuel format√©: {len(formatted)} caract√®res")
        return formatted

    ##############################################
    # M√©thode pour la synchronisation des stats #
    ##############################################
    def obtenir_etat_cognitif(self) -> Dict[str, Dict]:
        """
        Agr√©gateur de m√©triques pour le Dashboard.

        Collecte les statistiques de tous les sous-agents (Memoire, Recherche, LLM)
        et les normalise pour l'affichage frontend.
        G√®re les cas o√π un agent n'est pas initialis√© (Graceful degradation).

        Returns:
            Dict avec les stats de chaque agent au format:
            {
                "AgentMemoire": {
                    "appels_total": int,
                    "erreurs_total": int,
                    "temps_moyen_ms": float,
                    "stats_specifiques": dict
                },
                ...
            }
        """
        etat_cognitif = {}

        # Liste des agents √† interroger
        agents_a_interroger = [
            ("AgentMemoire", self.agent_memoire),
            ("AgentRecherche", self.agent_recherche),
            ("AgentContexte", self.agent_contexte),
            ("AgentParole", self.agent_parole),
            ("AgentJuge", self.agent_juge),
            ("AgentReflexor", self.agent_reflexor),
            ("MoteurLLM", self.moteur_llm),
            ("MoteurMiniLLM", self.moteur_mini_llm),
            ("IntentionDetector", self.intention_detector),
        ]

        for nom_agent, instance_agent in agents_a_interroger:
            try:
                # V√©rifier si l'agent a un stats_manager
                if (
                    hasattr(instance_agent, "stats_manager")
                    and instance_agent.stats_manager
                ):
                    # ‚úÖ L'ASTUCE EST ICI : On cr√©e une variable typ√©e
                    manager: StatsBase = instance_agent.stats_manager
                    stats = manager.obtenir_statistiques()
                    etat_cognitif[nom_agent] = {
                        "appels_total": stats.get("appels_total", 0),
                        "erreurs_total": stats.get("erreurs_total", 0),
                        "temps_moyen_ms": 0,  # StatsBase n'a pas temps_moyen_ms
                        "stats_specifiques": stats.get("stats_specifiques", {}),
                    }
                else:
                    # Fallback si pas de stats_manager
                    etat_cognitif[nom_agent] = {
                        "appels_total": 0,
                        "erreurs_total": 0,
                        "temps_moyen_ms": 0,
                        "stats_specifiques": {},
                    }
                    self.logger.log_warning(f"‚ö†Ô∏è {nom_agent} n'a pas de stats_manager")

            except Exception as e:
                self.logger.log_error(
                    f"Erreur lors de la collecte des stats pour {nom_agent}: {e}"
                )
                etat_cognitif[nom_agent] = {
                    "appels_total": 0,
                    "erreurs_total": 0,
                    "temps_moyen_ms": 0,
                    "stats_specifiques": {},
                    "erreur": str(e),
                }

        # Ajouter les stats propres de Semi
        try:
            if hasattr(self, "stats_manager") and self.stats_manager:
                stats_semi = self.stats_manager.obtenir_statistiques()
                etat_cognitif["AgentSemi"] = {
                    "appels_total": stats_semi.get("appels_total", 0),
                    "erreurs_total": stats_semi.get("erreurs_total", 0),
                    "temps_moyen_ms": 0,
                    "stats_specifiques": stats_semi.get("stats_specifiques", {}),
                }
        except Exception as e:
            self.logger.log_error(
                f"Erreur lors de la collecte des stats pour AgentSemi: {e}"
            )

        self.logger.info(f"üìä √âtat cognitif collect√© pour {len(etat_cognitif)} agents")
        return etat_cognitif


# --- ZONE DE TEST ARCHITECTE ---
if __name__ == "__main__":
    print("üöÄ D√©marrage du mode TEST ISOL√â pour Agent Semi...")

    try:
        # 1. Instanciation
        agent_test = AgentSemi()
        print(f"‚úÖ Agent pr√™t : {agent_test}")

        # 2. D√âCLENCHEMENT DU FLUX (Simulation d'une question utilisateur)
        print("üèÅ Envoi de la requ√™te 'Qui es-tu ?'...")

        # On appelle la m√©thode principale comme le ferait le serveur
        # Note : penser() est un g√©n√©rateur, il faut boucler dessus pour l'ex√©cuter
        generateur = agent_test.penser("Qui es-tu ?", stream=False)

        for bout_de_reponse in generateur:
            print(f"üì§ Sortie re√ßue : {bout_de_reponse}")

    except Exception as e:
        print(f"‚ùå ERREUR FLUX : {e}")
        raise e
