dashboard_stats:
  derniere_mise_a_jour: '2026-01-07 17:43:22'
  appels_total: 2
  erreurs_total: 0
  taux_de_succes: 100.00%
  specifiques:
    appels_generer_stream: 2
active_profile: qwen_coder

models:
  # ----------------------------------------------------------
  # PROFIL Mistral
  # ----------------------------------------------------------
  mistral_production:
    model_name: Mistral-7B-Instruct-v0.3
    # Le chemin exact que nous avons vu dans vos logs précédents
    model_path: D:/rag_personnel/model/mistral-7B-Instruct-v0.3.Q4_K_M.gguf

    loading:
      # RTX 3090 : On peut monter le contexte au max du modèle
      context_window: 32768
      # -1 signifie "Mettre toutes les couches sur le GPU" (Vitesse max)
      gpu_layers: -1
      n_batch: 2048
      n_threads: 8
      chat_format: chatml
      use_mmap: false
      use_mlock: true
      verbose: true

    generation:
      temperature: 0.7
      max_tokens: 2048
      top_p: 0.9
      stop_tokens: ["<|im_end|>", "</s>", "User:"]

  # ----------------------------------------------------------
  # 1️⃣  PROFIL Qwen 2.5 Instruct 14B AWQ
  # ----------------------------------------------------------
  qwen_14b:

    model_name: qwen2.5-14b-instruct-awq
    backend: transformers
    model_path: D:/rag_personnel/model/qwen2.5-14b-instruct-awq
    loading:
      base_model_path: D:/rag_personnel/model/qwen2.5-14b-instruct-awq
      context_window: 32768
      gpu_layers: 46 #Nombre de layers pour Qwen 2.5 = 48
      n_batch: 2048
      n_threads: 8
      chat_format: chatml
      use_mmap: true
      use_mlock: true
      verbose: false
      logits_all: false
      load_in_4bit: false
      device_map: "cuda"

    generation:
      temperature: 0.7
      max_tokens: 2048
      top_p: 0.95
      do_sample: false
      stop_tokens: ["<|im_end|>", "</s>", "User:"]

  # ----------------------------------------------------------
  # 1️⃣  PROFIL Qwen 2.5 Instruct 14B GGUF
  # ----------------------------------------------------------
  qwen_14b_gguf:

    model_name: model/Qwen2.5-14B-Instruct-Q4_K_M.gguf
    model_path: D:/rag_personnel/model/model/Qwen2.5-14B-Instruct-Q4_K_M.gguf
    server_url: "http://127.0.0.1:8080"
    loading:
      context_window: 32768 #65536 #131072
      n_gpu_layers: -1 #Nombre de layers pour Qwen 2.5 = 48
      cache_type_k: q8_0 #q8_0 q4_0
      cache_type_v: q8_0 #q8_0 q4_0
      n_batch: 2048
      n_threads: 8
      chat_format: chatml
      use_mmap: true
      use_mlock: true
      verbose: false
      logits_all: false
      flash_attn: true

    generation:
      temperature: 0.7
      max_tokens: 2048
      top_p: 0.95
      do_sample: false
      cache_prompt: false
      stop_tokens: ["<|im_end|>", "</s>", "User:"]


  # ----------------------------------------------------------
  # 1️⃣  PROFIL Qwen 2.5 CoderInstruct 14B
  # ----------------------------------------------------------
  ############ instruction-tuned 14B Qwen2.5 model specs.############
  #Type: Causal Language Models
  #Training Stage: Pretraining & Post-training
  #Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias
  #Number of Parameters: 14.7B
  #Number of Paramaters (Non-Embedding): 13.1B
  #Number of Layers: 48
  #Number of Attention Heads (GQA): 40 for Q and 8 for KV
  #Context Length: Full 131,072 tokens and generation 8192 tokens
  #tokenizer = AutoTokenizer.from_pretrained
  ###################################################################

  qwen_coder:

    model_name: Qwen_2.5_CoderInstruct_14B
    model_path: D:/rag_personnel/model/qwen2.5-coder-14b-instruct-q4_k_m.gguf
    server_url: "http://127.0.0.1:8080"

    loading:
      context_window: 32768 #65536 #131072
      n_gpu_layers: -1 #Nombre de layers pour Qwen 2.5 = 48
      cache_type_k: q8_0 #q8_0 q4_0
      cache_type_v: q8_0 #q8_0 q4_0
      n_batch: 2048
      n_threads: 8
      chat_format: chatml
      use_mmap: true
      use_mlock: true
      verbose: false
      logits_all: false
      flash_attn: true

    generation:
      temperature: 0.7
      max_tokens: 2048
      top_p: 0.95
      do_sample: false
      cache_prompt: false
      stop_tokens: ["<|im_end|>", "</s>", "User:"]


  # ----------------------------------------------------------
  # 2️⃣  PROFIL LORA — version fine-tunée (Transformers)
  # ----------------------------------------------------------
  main_llm_lora:
    model_name: Qwen_2.5_CoderInstruct_14B_LoRA
    model_path: D:/rag_personnel/model/qwen_2.5_CoderInstruct_14b
    # ⚠️ Lorsque tu auras entraîné ton LoRA principal, indique son chemin ici :
    # ex: D:/rag_personnel/data_training_center/Semi/scripts/main_llm_lora_adapter

    loading:
      base_model_path: D:/rag_personnel/model/qwen_2.5_CoderInstruct_14b
      lora_adapter_path: D:/rag_personnel/data_training_center/Semi/scripts/main_llm_lora_adapter
      device_map: cuda
      torch_dtype: float16
      attn_implementation: sdpa
      context_window: 8192
      gpu_layers: 40
      n_batch: 512
      n_threads: 8
      verbose: true

    generation:
      temperature: 0.6
      max_tokens: 1024
      top_p: 0.9
      do_sample: true
      stop_tokens: ["<|im_end|>", "</s>", "User:"]
