# RAG Code â€” SystÃ¨me de Recherche Intelligente

Recherche hybride (vectorielle + symbolique) sur le code source du projet, avec expansion automatique des dÃ©pendances.

```
Question utilisateur
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   AgentCode     â”‚  â† API publique (fournir_contexte)
   â”‚   Orchestrateur â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Recherche hybride
     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
     â–¼             â–¼
  FAISS      Graphe AST
 (vecteurs)  (dÃ©pendances)
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â–¼
   Contexte enrichi â†’ LLM
```

## Composants

| Fichier | RÃ´le | EntrÃ©e â†’ Sortie |
|---------|------|-----------------|
| `agent_Code.py` | Orchestrateur RAG | Question â†’ `List[ContexteCode]` |
| `moteur_vecteur_code.py` | Indexeur (scan AST + FAISS) | Projet â†’ Index disque |
| `code_extractor_manager.py` | Analyseur flux LLM | RÃ©ponse brute â†’ `ArtefactCode` |

## Pourquoi pas un RAG naÃ¯f ?

Un RAG classique dÃ©coupe le code par lignes ou tokens fixes, puis fait une recherche par similaritÃ©. RÃ©sultat : des chunks tronquÃ©s, sans contexte, qui forcent le LLM Ã  deviner.

### 1. Chunks sÃ©mantiques enrichis

Chaque chunk est une **unitÃ© logique** (fonction, classe, mÃ©thode) avec ses mÃ©tadonnÃ©es :

```python
ContexteCode(
    signature="def fournir_contexte(self, question: str, top_k: int = 8) -> List[Any]",
    docstring="MÃ©thode principale : reÃ§oit une question, retourne le contexte...",
    dependencies=["chercher_code", "_generer_squelette_partiel"],
    variables_used=["self.arch", "self.moteur_vecteur"],
    return_type="List[Any]",
    key_concepts=["rag", "contexte", "recherche"]
)
```

Le LLM reÃ§oit tout ce qu'il faut pour comprendre **ce que fait** la fonction, **ce qu'elle appelle**, et **ce qu'elle manipule**.

### 2. Expansion par graphe de dÃ©pendances

Une recherche sur "AgentMemoire" ne retourne pas que ce module. Le systÃ¨me :
- Parse les imports pour trouver les dÃ©pendances sortantes
- Remonte le graphe d'appels pour les dÃ©pendances entrantes
- Expand sur N niveaux de profondeur

RÃ©sultat : le LLM voit le **contexte d'intÃ©gration**, pas un fichier isolÃ©.

### 3. Squelette dynamique filtrÃ©

Au lieu d'injecter 50 fichiers, le systÃ¨me gÃ©nÃ¨re une vue arborescente **ciblÃ©e** :

```
ğŸ“¦ MODULE : agent_Memoire (agentique/agent_Memoire.py)
  â””â”€â”€ class AgentMemoire
      â””â”€â”€ def sauvegarder_souvenir
      â””â”€â”€ def rechercher_souvenirs
  â””â”€â”€ def _formater_contexte
```

Le LLM voit la structure du projet sans Ãªtre noyÃ© par le code complet.

### 4. Architecture scalable (Offsets JSONL)

- **En RAM** : Index FAISS + table d'offsets (position byte de chaque chunk)
- **Sur disque** : Chunks complets en JSONL

Ã€ la requÃªte, seuls les chunks pertinents sont lus via `seek()`. Pas de chargement de tout l'index en mÃ©moire.

## SÃ©paration Lecture/Ã‰criture

| Mode | Composant | ResponsabilitÃ© |
|------|-----------|----------------|
| **Lecture** | AgentCode | Interroge l'index, assemble le contexte |
| **Ã‰criture** | MoteurVecteurCode | Scan AST, gÃ©nÃ¨re chunks, construit FAISS |

Cette sÃ©paration permet un **hot-reload** : on peut reconstruire l'index en arriÃ¨re-plan sans interrompre les requÃªtes.
